{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"","slug":"appv1","date":"2017-10-27T02:10:47.904Z","updated":"2017-10-27T02:10:47.904Z","comments":true,"path":"2017/10/27/appv1/","link":"","permalink":"http://yoursite.com/2017/10/27/appv1/","excerpt":"","text":"appv1","categories":[],"tags":[]},{"title":"","slug":"README","date":"2017-10-27T02:10:47.903Z","updated":"2017-10-27T02:10:47.903Z","comments":true,"path":"2017/10/27/README/","link":"","permalink":"http://yoursite.com/2017/10/27/README/","excerpt":"","text":"git test","categories":[],"tags":[]},{"title":"","slug":"09-部署Dashboard插件","date":"2017-10-27T01:45:16.509Z","updated":"2017-10-27T01:54:07.287Z","comments":true,"path":"2017/10/27/09-部署Dashboard插件/","link":"","permalink":"http://yoursite.com/2017/10/27/09-部署Dashboard插件/","excerpt":"","text":"09-部署Dashboard插件tags: dashboard 部署 dashboard 插件官方文件目录：kubernetes/cluster/addons/dashboard 使用的文件： 12$ ls *.yamldashboard-controller.yaml dashboard-rbac.yaml dashboard-service.yaml 新加了 dashboard-rbac.yaml 文件，定义 dashboard 使用的 RoleBinding。 由于 kube-apiserver 启用了 RBAC 授权，而官方源码目录的 dashboard-controller.yaml 没有定义授权的 ServiceAccount，所以后续访问 kube-apiserver 的 API 时会被拒绝，前端界面提示： 解决办法是：定义一个名为 dashboard 的 ServiceAccount，然后将它和 Cluster Role view 绑定，具体参考 dashboard-rbac.yaml文件。 已经修改好的 yaml 文件见：dashboard。 配置dashboard-service123$ diff dashboard-service.yaml.orig dashboard-service.yaml10a11&gt; type: NodePort 指定端口类型为 NodePort，这样外界可以通过地址 nodeIP:nodePort 访问 dashboard； 配置dashboard-controller12345620a21&gt; serviceAccountName: dashboard23c24&lt; image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.6.0---&gt; image: cokabug/kubernetes-dashboard-amd64:v1.6.0 使用名为 dashboard 的自定义 ServiceAccount； 执行所有定义文件123456$ pwd/root/kubernetes/cluster/addons/dashboard$ ls *.yamldashboard-controller.yaml dashboard-rbac.yaml dashboard-service.yaml$ kubectl create -f .$ 检查执行结果查看分配的 NodePort 123$ kubectl get services kubernetes-dashboard -n kube-systemNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes-dashboard 10.254.224.130 &lt;nodes&gt; 80:30312/TCP 25s NodePort 30312映射到 dashboard pod 80端口； 检查 controller 12345$ kubectl get deployment kubernetes-dashboard -n kube-systemNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEkubernetes-dashboard 1 1 1 1 3m$ kubectl get pods -n kube-system | grep dashboardkubernetes-dashboard-1339745653-pmn6z 1/1 Running 0 4m 访问dashboard kubernetes-dashboard 服务暴露了 NodePort，可以使用 http://NodeIP:nodePort 地址访问 dashboard； 通过 kube-apiserver 访问 dashboard； 通过 kubectl proxy 访问 dashboard： 通过 kubectl proxy 访问 dashboard启动代理 12$ kubectl proxy --address='10.64.3.7' --port=8086 --accept-hosts='^*$'Starting to serve on 10.64.3.7:8086 需要指定 --accept-hosts 选项，否则浏览器访问 dashboard 页面时提示 “Unauthorized”； 浏览器访问 URL：http://10.64.3.7:8086/ui自动跳转到：http://10.64.3.7:8086/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard/#/workload?namespace=default 通过 kube-apiserver 访问dashboard获取集群服务地址列表 1234$ kubectl cluster-infoKubernetes master is running at https://10.64.3.7:6443KubeDNS is running at https://10.64.3.7:6443/api/v1/proxy/namespaces/kube-system/services/kube-dnskubernetes-dashboard is running at https://10.64.3.7:6443/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard 由于 kube-apiserver 开启了 RBAC 授权，而浏览器访问 kube-apiserver 的时候使用的是匿名证书，所以访问安全端口会导致授权失败。这里需要使用非安全端口访问 kube-apiserver： 浏览器访问 URL：http://10.64.3.7:8080/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard 由于缺少 Heapster 插件，当前 dashboard 不能展示 Pod、Nodes 的 CPU、内存等 metric 图形；","categories":[],"tags":[]},{"title":"","slug":"08-部署DNS插件","date":"2017-10-27T01:45:16.499Z","updated":"2017-10-27T01:53:56.748Z","comments":true,"path":"2017/10/27/08-部署DNS插件/","link":"","permalink":"http://yoursite.com/2017/10/27/08-部署DNS插件/","excerpt":"","text":"08-部署DNS插件tags: kubedns 部署 kubedns 插件官方文件目录：kubernetes/cluster/addons/dns 使用的文件： 12$ ls *.yaml *.basekubedns-cm.yaml kubedns-sa.yaml kubedns-controller.yaml.base kubedns-svc.yaml.base 已经修改好的 yaml 文件见：dns。 系统预定义的 RoleBinding预定义的 RoleBinding system:kube-dns 将 kube-system 命名空间的 kube-dns ServiceAccount 与 system:kube-dns Role 绑定， 该 Role 具有访问 kube-apiserver DNS 相关 API 的权限； 123456789101112131415161718192021$ kubectl get clusterrolebindings system:kube-dns -o yamlapiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" creationTimestamp: 2017-04-06T17:40:47Z labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-dns resourceVersion: \"56\" selfLink: /apis/rbac.authorization.k8s.io/v1beta1/clusterrolebindingssystem%3Akube-dns uid: 2b55cdbe-1af0-11e7-af35-8cdcd4b3be48roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-dnssubjects:- kind: ServiceAccount name: kube-dns namespace: kube-system kubedns-controller.yaml 中定义的 Pods 时使用了 kubedns-sa.yaml 文件定义的 kube-dns ServiceAccount，所以具有访问 kube-apiserver DNS 相关 API 的权限； 配置 kube-dns ServiceAccount无需修改； 配置 kube-dns 服务12345$ diff kubedns-svc.yaml.base kubedns-svc.yaml30c30&lt; clusterIP: __PILLAR__DNS__SERVER__---&gt; clusterIP: 10.254.0.2 需要将 spec.clusterIP 设置为集群环境变量中变量 CLUSTER_DNS_SVC_IP 值，这个 IP 需要和 kubelet 的 —cluster-dns 参数值一致； 配置 kube-dns Deployment12345678910111213141516171819202122232425262728293031$ diff kubedns-controller.yaml.base kubedns-controller.yaml58c58&lt; image: gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.1---&gt; image: xuejipeng/k8s-dns-kube-dns-amd64:v1.14.188c88&lt; - --domain=__PILLAR__DNS__DOMAIN__.---&gt; - --domain=cluster.local.92c92&lt; __PILLAR__FEDERATIONS__DOMAIN__MAP__---&gt; #__PILLAR__FEDERATIONS__DOMAIN__MAP__110c110&lt; image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.1---&gt; image: xuejipeng/k8s-dns-dnsmasq-nanny-amd64:v1.14.1129c129&lt; - --server=/__PILLAR__DNS__DOMAIN__/127.0.0.1#10053---&gt; - --server=/cluster.local./127.0.0.1#10053148c148&lt; image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.1---&gt; image: xuejipeng/k8s-dns-sidecar-amd64:v1.14.1161,162c161,162&lt; - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.__PILLAR__DNS__DOMAIN__,5,A&lt; - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.__PILLAR__DNS__DOMAIN__,5,A---&gt; - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local.,5,A&gt; - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local.,5,A --domain 为集群环境文档 变量 CLUSTER_DNS_DOMAIN 的值； 使用系统已经做了 RoleBinding 的 kube-dns ServiceAccount，该账户具有访问 kube-apiserver DNS 相关 API 的权限； 执行所有定义文件123456$ pwd/root/kubernetes-git/cluster/addons/dns$ ls *.yamlkubedns-cm.yaml kubedns-controller.yaml kubedns-sa.yaml kubedns-svc.yaml$ kubectl create -f .$ 检查 kubedns 功能新建一个 Deployment 12345678910111213141516171819$ cat my-nginx.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: my-nginxspec: replicas: 2 template: metadata: labels: run: my-nginx spec: containers: - name: my-nginx image: nginx:1.7.9 ports: - containerPort: 80$ kubectl create -f my-nginx.yaml$ Export 该 Deployment, 生成 my-nginx 服务 123$ kubectl expose deploy my-nginx$ kubectl get services --all-namespaces |grep my-nginxdefault my-nginx 10.254.86.48 &lt;none&gt; 80/TCP 1d 创建另一个 Pod，查看 /etc/resolv.conf 是否包含 kubelet 配置的 --cluster_dns 和 --cluster_domain，是否能够将服务 my-nginx 解析到上面显示的 Cluster IP 10.254.86.48 1234567891011121314151617181920212223242526272829303132$ cat pod-nginx.yamlapiVersion: v1kind: Podmetadata: name: nginxspec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80$ kubectl create -f pod-nginx.yaml$ kubectl exec nginx -i -t -- /bin/bashroot@nginx:/# cat /etc/resolv.confnameserver 10.254.0.2search default.svc.cluster.local svc.cluster.local cluster.local tjwq01.ksyun.comoptions ndots:5root@nginx:/# ping my-nginxPING my-nginx.default.svc.cluster.local (10.254.86.48): 48 data bytes^C--- my-nginx.default.svc.cluster.local ping statistics ---2 packets transmitted, 0 packets received, 100% packet lossroot@nginx:/# ping kubernetesPING kubernetes.default.svc.cluster.local (10.254.0.1): 48 data bytes^C--- kubernetes.default.svc.cluster.local ping statistics ---1 packets transmitted, 0 packets received, 100% packet lossroot@nginx:/# ping kube-dns.kube-system.svc.cluster.localPING kube-dns.kube-system.svc.cluster.local (10.254.0.2): 48 data bytes^C--- kube-dns.kube-system.svc.cluster.local ping statistics ---1 packets transmitted, 0 packets received, 100% packet loss","categories":[],"tags":[]},{"title":"","slug":"07-部署Node节点","date":"2017-10-27T01:45:16.490Z","updated":"2017-10-27T01:53:43.429Z","comments":true,"path":"2017/10/27/07-部署Node节点/","link":"","permalink":"http://yoursite.com/2017/10/27/07-部署Node节点/","excerpt":"","text":"07-部署Node节点tags: node, flanneld, docker, kubeconfig, kubelet, kube-proxy 部署 Node 节点kubernetes Node 节点包含如下组件： flanneld docker kubelet kube-proxy 使用的变量本文档用到的变量定义如下： 12345678$ # 替换为 kubernetes master 集群任一机器 IP$ export MASTER_IP=10.64.3.7$ export KUBE_APISERVER=\"https://$&#123;MASTER_IP&#125;:6443\"$ # 当前部署的节点 IP$ export NODE_IP=10.64.3.7$ # 导入用到的其它全局变量：ETCD_ENDPOINTS、FLANNEL_ETCD_PREFIX、CLUSTER_CIDR、CLUSTER_DNS_SVC_IP、CLUSTER_DNS_DOMAIN、SERVICE_CIDR$ source /root/local/bin/environment.sh$ 安装和配置 flanneld参考 05-部署Flannel网络.md 安装和配置 docker下载最新的 docker 二进制文件12345$ wget https://get.docker.com/builds/Linux/x86_64/docker-17.04.0-ce.tgz$ tar -xvf docker-17.04.0-ce.tgz$ cp docker/docker* /root/local/bin$ cp docker/completion/bash/docker /etc/bash_completion.d/$ 创建 docker 的 systemd unit 文件1234567891011121314151617181920$ cat docker.service[Unit]Description=Docker Application Container EngineDocumentation=http://docs.docker.io[Service]Environment=\"PATH=/root/local/bin:/bin:/sbin:/usr/bin:/usr/sbin\"EnvironmentFile=-/run/flannel/dockerExecStart=/root/local/bin/dockerd --log-level=error $DOCKER_NETWORK_OPTIONSExecReload=/bin/kill -s HUP $MAINPIDRestart=on-failureRestartSec=5LimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinityDelegate=yesKillMode=process[Install]WantedBy=multi-user.target dockerd 运行时会调用其它 docker 命令，如 docker-proxy，所以需要将 docker 命令所在的目录加到 PATH 环境变量中； flanneld 启动时将网络配置写入到 /run/flannel/docker 文件中的变量 DOCKER_NETWORK_OPTIONS，dockerd 命令行上指定该变量值来设置 docker0 网桥参数； 如果指定了多个 EnvironmentFile 选项，则必须将 /run/flannel/docker 放在最后(确保 docker0 使用 flanneld 生成的 bip 参数)； 不能关闭默认开启的 --iptables 和 --ip-masq 选项； 如果内核版本比较新，建议使用 overlay 存储驱动； docker 从 1.13 版本开始，可能将 iptables FORWARD chain的默认策略设置为DROP，从而导致 ping 其它 Node 上的 Pod IP 失败，遇到这种情况时，需要手动设置策略为 ACCEPT： 12$ sudo iptables -P FORWARD ACCEPT$ 为了加快 pull image 的速度，可以使用国内的仓库镜像服务器，同时增加下载的并发数。(如果 dockerd 已经运行，则需要重启 dockerd 生效。) 12345$ cat /etc/docker/daemon.json&#123; \"registry-mirrors\": [\"https://docker.mirrors.ustc.edu.cn\", \"hub-mirror.c.163.com\"], \"max-concurrent-downloads\": 10&#125; 完整 unit 见 docker.service 启动 dockerd12345678$ sudo cp docker.service /etc/systemd/system/docker.service$ sudo systemctl daemon-reload$ sudo systemctl stop firewalld$ sudo systemctl disable firewalld$ sudo iptables -F &amp;&amp; sudo iptables -X &amp;&amp; sudo iptables -F -t nat &amp;&amp; sudo iptables -X -t nat$ sudo systemctl enable docker$ sudo systemctl start docker$ 需要关闭 firewalld，否则可能会重复创建的 iptables 规则； 最好清理旧的 iptables rules 和 chains 规则； 检查 docker 服务12$ docker version$ 安装和配置 kubeletkubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper 角色，然后 kubelet 才有权限创建认证请求(certificatesigningrequests)： 12$ kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap$ --user=kubelet-bootstrap 是文件 /etc/kubernetes/token.csv 中指定的用户名，同时也写入了文件 /etc/kubernetes/bootstrap.kubeconfig； 下载最新的 kubelet 和 kube-proxy 二进制文件123456$ wget https://dl.k8s.io/v1.6.2/kubernetes-server-linux-amd64.tar.gz$ tar -xzvf kubernetes-server-linux-amd64.tar.gz$ cd kubernetes$ tar -xzvf kubernetes-src.tar.gz$ sudo cp -r ./server/bin/&#123;kube-proxy,kubelet&#125; /root/local/bin/$ 创建 kubelet bootstrapping kubeconfig 文件123456789101112131415161718$ # 设置集群参数$ kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=bootstrap.kubeconfig$ # 设置客户端认证参数$ kubectl config set-credentials kubelet-bootstrap \\ --token=$&#123;BOOTSTRAP_TOKEN&#125; \\ --kubeconfig=bootstrap.kubeconfig$ # 设置上下文参数$ kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig$ # 设置默认上下文$ kubectl config use-context default --kubeconfig=bootstrap.kubeconfig$ mv bootstrap.kubeconfig /etc/kubernetes/ --embed-certs 为 true 时表示将 certificate-authority 证书写入到生成的 bootstrap.kubeconfig 文件中； 设置 kubelet 客户端认证参数时没有指定秘钥和证书，后续由 kube-apiserver 自动生成； 创建 kubelet 的 systemd unit 文件1234567891011121314151617181920212223242526272829303132333435$ sudo mkdir /var/lib/kubelet # 必须先创建工作目录$ cat &gt; kubelet.service &lt;&lt;EOF[Unit]Description=Kubernetes KubeletDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletExecStart=/root/local/bin/kubelet \\\\ --address=$&#123;NODE_IP&#125; \\\\ --hostname-override=$&#123;NODE_IP&#125; \\\\ --pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest \\\\ --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\\\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\\\ --require-kubeconfig \\\\ --cert-dir=/etc/kubernetes/ssl \\\\ --cluster_dns=$&#123;CLUSTER_DNS_SVC_IP&#125; \\\\ --cluster_domain=$&#123;CLUSTER_DNS_DOMAIN&#125; \\\\ --hairpin-mode promiscuous-bridge \\\\ --allow-privileged=true \\\\ --serialize-image-pulls=false \\\\ --logtostderr=true \\\\ --v=2ExecStopPost=/sbin/iptables -A INPUT -s 10.0.0.0/8 -p tcp --dport 4194 -j ACCEPTExecStopPost=/sbin/iptables -A INPUT -s 172.16.0.0/12 -p tcp --dport 4194 -j ACCEPTExecStopPost=/sbin/iptables -A INPUT -s 192.168.0.0/16 -p tcp --dport 4194 -j ACCEPTExecStopPost=/sbin/iptables -A INPUT -p tcp --dport 4194 -j DROPRestart=on-failureRestartSec=5[Install]WantedBy=multi-user.targetEOF --address 不能设置为 127.0.0.1，否则后续 Pods 访问 kubelet 的 API 接口时会失败，因为 Pods 访问的 127.0.0.1 指向自己而不是 kubelet； 如果设置了 --hostname-override 选项，则 kube-proxy 也需要设置该选项，否则会出现找不到 Node 的情况； --experimental-bootstrap-kubeconfig 指向 bootstrap kubeconfig 文件，kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求； 管理员通过了 CSR 请求后，kubelet 自动在 --cert-dir 目录创建证书和私钥文件(kubelet-client.crt 和 kubelet-client.key)，然后写入 --kubeconfig 文件(自动创建 --kubeconfig 指定的文件)； 建议在 --kubeconfig 配置文件中指定 kube-apiserver 地址，如果未指定 --api-servers 选项，则必须指定 --require-kubeconfig 选项后才从配置文件中读取 kue-apiserver 的地址，否则 kubelet 启动后将找不到 kube-apiserver (日志中提示未找到 API Server），kubectl get nodes 不会返回对应的 Node 信息; --cluster_dns 指定 kubedns 的 Service IP(可以先分配，后续创建 kubedns 服务时指定该 IP)，--cluster_domain 指定域名后缀，这两个参数同时指定后才会生效； kubelet cAdvisor 默认在所有接口监听 4194 端口的请求，对于有外网的机器来说不安全，ExecStopPost 选项指定的 iptables 规则只允许内网机器访问 4194 端口； 完整 unit 见 kubelet.service 启动 kubelet123456$ sudo cp kubelet.service /etc/systemd/system/kubelet.service$ sudo systemctl daemon-reload$ sudo systemctl enable kubelet$ sudo systemctl start kubelet$ systemctl status kubelet$ 通过 kubelet 的 TLS 证书请求kubelet 首次启动时向 kube-apiserver 发送证书签名请求，必须通过后 kubernetes 系统才会将该 Node 加入到集群。 查看未授权的 CSR 请求： 12345$ kubectl get csrNAME AGE REQUESTOR CONDITIONcsr-2b308 4m kubelet-bootstrap Pending$ kubectl get nodesNo resources found. 通过 CSR 请求： 12345$ kubectl certificate approve csr-2b308certificatesigningrequest \"csr-2b308\" approved$ kubectl get nodesNAME STATUS AGE VERSION10.64.3.7 Ready 49m v1.6.2 自动生成了 kubelet kubeconfig 文件和公私钥： 1234567$ ls -l /etc/kubernetes/kubelet.kubeconfig-rw------- 1 root root 2284 Apr 7 02:07 /etc/kubernetes/kubelet.kubeconfig$ ls -l /etc/kubernetes/ssl/kubelet*-rw-r--r-- 1 root root 1046 Apr 7 02:07 /etc/kubernetes/ssl/kubelet-client.crt-rw------- 1 root root 227 Apr 7 02:04 /etc/kubernetes/ssl/kubelet-client.key-rw-r--r-- 1 root root 1103 Apr 7 02:07 /etc/kubernetes/ssl/kubelet.crt-rw------- 1 root root 1675 Apr 7 02:07 /etc/kubernetes/ssl/kubelet.key 配置 kube-proxy创建 kube-proxy 证书创建 kube-proxy 证书签名请求： 123456789101112131415161718$ cat kube-proxy-csr.json&#123; \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125; CN 指定该证书的 User 为 system:kube-proxy； kube-apiserver 预定义的 RoleBinding system:node-proxier 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限； hosts 属性值为空列表； 生成 kube-proxy 客户端证书和私钥： 123456789$ cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/etc/kubernetes/ssl/ca-config.json \\ -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy$ ls kube-proxy*kube-proxy.csr kube-proxy-csr.json kube-proxy-key.pem kube-proxy.pem$ sudo mv kube-proxy*.pem /etc/kubernetes/ssl/$ rm kube-proxy.csr kube-proxy-csr.json$ 创建 kube-proxy kubeconfig 文件1234567891011121314151617181920$ # 设置集群参数$ kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=kube-proxy.kubeconfig$ # 设置客户端认证参数$ kubectl config set-credentials kube-proxy \\ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig$ # 设置上下文参数$ kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig$ # 设置默认上下文$ kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig$ mv kube-proxy.kubeconfig /etc/kubernetes/ 设置集群参数和客户端认证参数时 --embed-certs 都为 true，这会将 certificate-authority、client-certificate 和 client-key 指向的证书文件内容写入到生成的 kube-proxy.kubeconfig 文件中； kube-proxy.pem 证书中 CN 为 system:kube-proxy，kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限； 创建 kube-proxy 的 systemd unit 文件1234567891011121314151617181920212223$ sudo mkdir -p /var/lib/kube-proxy # 必须先创建工作目录$ cat &gt; kube-proxy.service &lt;&lt;EOF[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]WorkingDirectory=/var/lib/kube-proxyExecStart=/root/local/bin/kube-proxy \\\\ --bind-address=$&#123;NODE_IP&#125; \\\\ --hostname-override=$&#123;NODE_IP&#125; \\\\ --cluster-cidr=$&#123;SERVICE_CIDR&#125; \\\\ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \\\\ --logtostderr=true \\\\ --v=2Restart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF --hostname-override 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 iptables 规则； --cluster-cidr 必须与 kube-apiserver 的 --service-cluster-ip-range 选项值一致； kube-proxy 根据 --cluster-cidr 判断集群内部和外部流量，指定 --cluster-cidr 或 --masquerade-all 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT； --kubeconfig 指定的配置文件嵌入了 kube-apiserver 的地址、用户名、证书、秘钥等请求和认证信息； 预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限； 完整 unit 见 kube-proxy.service 启动 kube-proxy123456$ sudo cp kube-proxy.service /etc/systemd/system/$ sudo systemctl daemon-reload$ sudo systemctl enable kube-proxy$ sudo systemctl start kube-proxy$ systemctl status kube-proxy$ 验证集群功能定义文件： 1234567891011121314151617181920212223242526272829303132333435$ cat nginx-ds.ymlapiVersion: v1kind: Servicemetadata: name: nginx-ds labels: app: nginx-dsspec: type: NodePort selector: app: nginx-ds ports: - name: http port: 80 targetPort: 80---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: nginx-ds labels: addonmanager.kubernetes.io/mode: Reconcilespec: template: metadata: labels: app: nginx-ds spec: containers: - name: my-nginx image: nginx:1.7.9 ports: - containerPort: 80 创建 Pod 和服务： 123$ kubectl create -f nginx-ds.ymlservice \"nginx-ds\" createddaemonset \"nginx-ds\" created 检查节点状态1234$ kubectl get nodesNAME STATUS AGE VERSION10.64.3.7 Ready 8d v1.6.210.64.3.8 Ready 8d v1.6.2 都为 Ready 时正常。 检查各 Node 上的 Pod IP 连通性123$ kubectl get pods -o wide|grep nginx-dsnginx-ds-6ktz8 1/1 Running 0 5m 172.30.25.19 10.64.3.7nginx-ds-6ktz9 1/1 Running 0 5m 172.30.20.20 10.64.3.8 可见，nginx-ds 的 Pod IP 分别是 172.30.25.19、172.30.20.20，在所有 Node 上分别 ping 这两个 IP，看是否连通。 检查服务 IP 和端口可达性12$ kubectl get svc |grep nginx-dsnginx-ds 10.254.136.178 &lt;nodes&gt; 80:8744/TCP 11m 可见： 服务IP：10.254.136.178 服务端口：80 NodePort端口：8744 在所有 Node 上执行： 12$ curl 10.254.136.178 # `kubectl get svc |grep nginx-ds` 输出中的服务 IP$ 预期输出 nginx 欢迎页面内容。 检查服务的 NodePort 可达性在所有 Node 上执行： 1234$ export NODE_IP=10.64.3.7 # 当前 Node 的 IP$ export NODE_PORT=8744 # `kubectl get svc |grep nginx-ds` 输出中 80 端口映射的 NodePort$ curl $&#123;NODE_IP&#125;:$&#123;NODE_PORT&#125;$ 预期输出 nginx 欢迎页面内容。","categories":[],"tags":[]},{"title":"","slug":"06-部署Master节点","date":"2017-10-27T01:45:16.480Z","updated":"2017-10-27T01:53:36.177Z","comments":true,"path":"2017/10/27/06-部署Master节点/","link":"","permalink":"http://yoursite.com/2017/10/27/06-部署Master节点/","excerpt":"","text":"06-部署Master节点tags: master, kube-apiserver, kube-scheduler, kube-controller-manager 部署 master 节点kubernetes master 节点包含的组件： kube-apiserver kube-scheduler kube-controller-manager 目前这三个组件需要部署在同一台机器上： kube-scheduler、kube-controller-manager 和 kube-apiserver 三者的功能紧密相关； 同时只能有一个 kube-scheduler、kube-controller-manager 进程处于工作状态，如果运行多个，则需要通过选举产生一个 leader； 本文档介绍部署单机 kubernetes master 节点的步骤，没有实现高可用 master 集群。 计划后续再介绍部署 LB 的步骤，客户端 (kubectl、kubelet、kube-proxy) 使用 LB 的 VIP 来访问 kube-apiserver，从而实现高可用 master 集群。 master 节点与 node 节点上的 Pods 通过 Pod 网络通信，所以需要在 master 节点上部署 Flannel 网络。 使用的变量本文档用到的变量定义如下： 1234$ export MASTER_IP=10.64.3.7 # 替换为当前部署的 master 机器 IP$ # 导入用到的其它全局变量：SERVICE_CIDR、CLUSTER_CIDR、NODE_PORT_RANGE、ETCD_ENDPOINTS、BOOTSTRAP_TOKEN$ source /root/local/bin/environment.sh$ 下载最新版本的二进制文件有两种下载方式： 从 github release 页面 下载发布版 tarball，解压后再执行下载脚本 123456$ wget https://github.com/kubernetes/kubernetes/releases/download/v1.6.2/kubernetes.tar.gz$ tar -xzvf kubernetes.tar.gz...$ cd kubernetes$ ./cluster/get-kube-binaries.sh... 从 CHANGELOG页面 下载 client 或 server tarball 文件 server 的 tarball kubernetes-server-linux-amd64.tar.gz 已经包含了 client(kubectl) 二进制文件，所以不用单独下载kubernetes-client-linux-amd64.tar.gz文件； 123456$ # wget https://dl.k8s.io/v1.6.2/kubernetes-client-linux-amd64.tar.gz$ wget https://dl.k8s.io/v1.6.2/kubernetes-server-linux-amd64.tar.gz$ tar -xzvf kubernetes-server-linux-amd64.tar.gz...$ cd kubernetes$ tar -xzvf kubernetes-src.tar.gz 将二进制文件拷贝到指定路径： 12$ sudo cp -r server/bin/&#123;kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet&#125; /root/local/bin/$ 安装和配置 flanneld参考 05-部署Flannel网络.md 创建 kubernetes 证书创建 kubernetes 证书签名请求 12345678910111213141516171819202122232425262728$ cat &gt; kubernetes-csr.json &lt;&lt;EOF&#123; \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"$&#123;MASTER_IP&#125;\", \"$&#123;CLUSTER_KUBERNETES_SVC_IP&#125;\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125;EOF 如果 hosts 字段不为空则需要指定授权使用该证书的 IP 或域名列表，所以上面分别指定了当前部署的 master 节点主机 IP； 还需要添加 kube-apiserver 注册的名为 kubernetes 的服务 IP (Service Cluster IP)，一般是 kube-apiserver --service-cluster-ip-range 选项值指定的网段的第一个IP，如 “10.254.0.1”； 123$ kubectl get svc kubernetesNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes 10.254.0.1 &lt;none&gt; 443/TCP 1d 生成 kubernetes 证书和私钥 123456789$ cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/etc/kubernetes/ssl/ca-config.json \\ -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes$ ls kubernetes*kubernetes.csr kubernetes-csr.json kubernetes-key.pem kubernetes.pem$ sudo mkdir -p /etc/kubernetes/ssl/$ sudo mv kubernetes*.pem /etc/kubernetes/ssl/$ rm kubernetes.csr kubernetes-csr.json 配置和启动 kube-apiserver创建 kube-apiserver 使用的客户端 token 文件kubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求，kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token.csv 一致，如果一致则自动为 kubelet生成证书和秘钥。 123456$ # 导入的 environment.sh 文件定义了 BOOTSTRAP_TOKEN 变量$ cat &gt; token.csv &lt;&lt;EOF$&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\"EOF$ mv token.csv /etc/kubernetes/$ 创建 kube-apiserver 的 systemd unit 文件1234567891011121314151617181920212223242526272829303132333435363738394041424344$ cat &gt; kube-apiserver.service &lt;&lt;EOF[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]ExecStart=/root/local/bin/kube-apiserver \\\\ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\\\ --advertise-address=$&#123;MASTER_IP&#125; \\\\ --bind-address=$&#123;MASTER_IP&#125; \\\\ --insecure-bind-address=$&#123;MASTER_IP&#125; \\\\ --authorization-mode=RBAC \\\\ --runtime-config=rbac.authorization.k8s.io/v1alpha1 \\\\ --kubelet-https=true \\\\ --experimental-bootstrap-token-auth \\\\ --token-auth-file=/etc/kubernetes/token.csv \\\\ --service-cluster-ip-range=$&#123;SERVICE_CIDR&#125; \\\\ --service-node-port-range=$&#123;NODE_PORT_RANGE&#125; \\\\ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\\\ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\\\ --client-ca-file=/etc/kubernetes/ssl/ca.pem \\\\ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\\\ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\\\ --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \\\\ --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \\\\ --etcd-servers=$&#123;ETCD_ENDPOINTS&#125; \\\\ --enable-swagger-ui=true \\\\ --allow-privileged=true \\\\ --apiserver-count=3 \\\\ --audit-log-maxage=30 \\\\ --audit-log-maxbackup=3 \\\\ --audit-log-maxsize=100 \\\\ --audit-log-path=/var/lib/audit.log \\\\ --event-ttl=1h \\\\ --v=2Restart=on-failureRestartSec=5Type=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF kube-apiserver 1.6 版本开始使用 etcd v3 API 和存储格式； --authorization-mode=RBAC 指定在安全端口使用 RBAC 授权模式，拒绝未通过授权的请求； kube-scheduler、kube-controller-manager 一般和 kube-apiserver 部署在同一台机器上，它们使用非安全端口和 kube-apiserver通信; kubelet、kube-proxy、kubectl 部署在其它 Node 节点上，如果通过安全端口访问 kube-apiserver，则必须先通过 TLS 证书认证，再通过 RBAC 授权； kube-proxy、kubectl 通过在使用的证书里指定相关的 User、Group 来达到通过 RBAC 授权的目的； 如果使用了 kubelet TLS Boostrap 机制，则不能再指定 --kubelet-certificate-authority、--kubelet-client-certificate 和 --kubelet-client-key 选项，否则后续 kube-apiserver 校验 kubelet 证书时出现 ”x509: certificate signed by unknown authority“ 错误； --admission-control 值必须包含 ServiceAccount，否则部署集群插件时会失败； --bind-address 不能为 127.0.0.1； --service-cluster-ip-range 指定 Service Cluster IP 地址段，该地址段不能路由可达； --service-node-port-range=${NODE_PORT_RANGE} 指定 NodePort 的端口范围； 缺省情况下 kubernetes 对象保存在 etcd /registry 路径下，可以通过 --etcd-prefix 参数进行调整； 完整 unit 见 kube-apiserver.service 启动 kube-apiserver123456$ sudo cp kube-apiserver.service /etc/systemd/system/$ sudo systemctl daemon-reload$ sudo systemctl enable kube-apiserver$ sudo systemctl start kube-apiserver$ sudo systemctl status kube-apiserver$ 配置和启动 kube-controller-manager创建 kube-controller-manager 的 systemd unit 文件12345678910111213141516171819202122232425$ cat &gt; kube-controller-manager.service &lt;&lt;EOF[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]ExecStart=/root/local/bin/kube-controller-manager \\\\ --address=127.0.0.1 \\\\ --master=http://$&#123;MASTER_IP&#125;:8080 \\\\ --allocate-node-cidrs=true \\\\ --service-cluster-ip-range=$&#123;SERVICE_CIDR&#125; \\\\ --cluster-cidr=$&#123;CLUSTER_CIDR&#125; \\\\ --cluster-name=kubernetes \\\\ --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\\\ --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\\\ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\\\ --root-ca-file=/etc/kubernetes/ssl/ca.pem \\\\ --leader-elect=true \\\\ --v=2Restart=on-failureRestartSec=5[Install]WantedBy=multi-user.targetEOF --address 值必须为 127.0.0.1，因为当前 kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器，否则： 1234$ kubectl get componentstatusesNAME STATUS MESSAGE ERRORcontroller-manager Unhealthy Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: getsockopt: connection refusedscheduler Unhealthy Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: getsockopt: connection refused 参考：https://github.com/kubernetes-incubator/bootkube/issues/64 --master=http://{MASTER_IP}:8080：使用非安全 8080 端口与 kube-apiserver 通信； --cluster-cidr 指定 Cluster 中 Pod 的 CIDR 范围，该网段在各 Node 间必须路由可达(flanneld保证)； --service-cluster-ip-range 参数指定 Cluster 中 Service 的CIDR范围，该网络在各 Node 间必须路由不可达，必须和 kube-apiserver 中的参数一致； --cluster-signing-* 指定的证书和私钥文件用来签名为 TLS BootStrap 创建的证书和私钥； --root-ca-file 用来对 kube-apiserver 证书进行校验，指定该参数后，才会在Pod 容器的 ServiceAccount 中放置该 CA 证书文件； --leader-elect=true 部署多台机器组成的 master 集群时选举产生一处于工作状态的 kube-controller-manager 进程； 完整 unit 见 kube-controller-manager.service 启动 kube-controller-manager12345$ sudo cp kube-controller-manager.service /etc/systemd/system/$ sudo systemctl daemon-reload$ sudo systemctl enable kube-controller-manager$ sudo systemctl start kube-controller-manager$ 配置和启动 kube-scheduler创建 kube-scheduler 的 systemd unit 文件1234567891011121314151617$ cat &gt; kube-scheduler.service &lt;&lt;EOF[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]ExecStart=/root/local/bin/kube-scheduler \\\\ --address=127.0.0.1 \\\\ --master=http://$&#123;MASTER_IP&#125;:8080 \\\\ --leader-elect=true \\\\ --v=2Restart=on-failureRestartSec=5[Install]WantedBy=multi-user.targetEOF --address 值必须为 127.0.0.1，因为当前 kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器； --master=http://{MASTER_IP}:8080：使用非安全 8080 端口与 kube-apiserver 通信； --leader-elect=true 部署多台机器组成的 master 集群时选举产生一处于工作状态的 kube-controller-manager 进程； 完整 unit 见 kube-scheduler.service。 启动 kube-scheduler12345$ sudo cp kube-scheduler.service /etc/systemd/system/$ sudo systemctl daemon-reload$ sudo systemctl enable kube-scheduler$ sudo systemctl start kube-scheduler$ 验证 master 节点功能1234567$ kubectl get componentstatusesNAME STATUS MESSAGE ERRORcontroller-manager Healthy okscheduler Healthy oketcd-0 Healthy &#123;\"health\": \"true\"&#125;etcd-1 Healthy &#123;\"health\": \"true\"&#125;etcd-2 Healthy &#123;\"health\": \"true\"&#125;","categories":[],"tags":[]},{"title":"","slug":"05-部署Flannel网络","date":"2017-10-27T01:45:16.467Z","updated":"2017-10-27T01:53:28.033Z","comments":true,"path":"2017/10/27/05-部署Flannel网络/","link":"","permalink":"http://yoursite.com/2017/10/27/05-部署Flannel网络/","excerpt":"","text":"05-部署Flannel网络tags: flanneld 部署 Flannel 网络kubernetes 要求集群内各节点能通过 Pod 网段互联互通，本文档介绍使用 Flannel 在所有节点 (Master、Node) 上创建互联互通的 Pod 网段的步骤。 使用的变量本文档用到的变量定义如下： 1234$ export NODE_IP=10.64.3.7 # 当前部署节点的 IP$ # 导入用到的其它全局变量：ETCD_ENDPOINTS、FLANNEL_ETCD_PREFIX、CLUSTER_CIDR$ source /root/local/bin/environment.sh$ 创建 TLS 秘钥和证书etcd 集群启用了双向 TLS 认证，所以需要为 flanneld 指定与 etcd 集群通信的 CA 和秘钥。 创建 flanneld 证书签名请求： 12345678910111213141516171819$ cat &gt; flanneld-csr.json &lt;&lt;EOF&#123; \"CN\": \"flanneld\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125;EOF hosts 字段为空； 生成 flanneld 证书和私钥： 123456789$ cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/etc/kubernetes/ssl/ca-config.json \\ -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld$ ls flanneld*flanneld.csr flanneld-csr.json flanneld-key.pem flanneld.pem$ sudo mkdir -p /etc/flanneld/ssl$ sudo mv flanneld*.pem /etc/flanneld/ssl$ rm flanneld.csr flanneld-csr.json 向 etcd 写入集群 Pod 网段信息注意：本步骤只需在第一次部署 Flannel 网络时执行，后续在其它节点上部署 Flannel 时无需再写入该信息！ 123456$ /root/local/bin/etcdctl \\ --endpoints=$&#123;ETCD_ENDPOINTS&#125; \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --cert-file=/etc/flanneld/ssl/flanneld.pem \\ --key-file=/etc/flanneld/ssl/flanneld-key.pem \\ set $&#123;FLANNEL_ETCD_PREFIX&#125;/config '&#123;\"Network\":\"'$&#123;CLUSTER_CIDR&#125;'\", \"SubnetLen\": 24, \"Backend\": &#123;\"Type\": \"vxlan\"&#125;&#125;' flanneld 目前版本 (v0.7.1) 不支持 etcd v3，故使用 etcd v2 API 写入配置 key 和网段数据； 写入的 Pod 网段(${CLUSTER_CIDR}，172.30.0.0/16) 必须与 kube-controller-manager 的 --cluster-cidr 选项值一致； 安装和配置 flanneld下载 flanneld12345$ mkdir flannel$ wget https://github.com/coreos/flannel/releases/download/v0.7.1/flannel-v0.7.1-linux-amd64.tar.gz$ tar -xzvf flannel-v0.7.1-linux-amd64.tar.gz -C flannel$ sudo cp flannel/&#123;flanneld,mk-docker-opts.sh&#125; /root/local/bin$ 创建 flanneld 的 systemd unit 文件123456789101112131415161718192021222324$ cat &gt; flanneld.service &lt;&lt; EOF[Unit]Description=Flanneld overlay address etcd agentAfter=network.targetAfter=network-online.targetWants=network-online.targetAfter=etcd.serviceBefore=docker.service[Service]Type=notifyExecStart=/root/local/bin/flanneld \\\\ -etcd-cafile=/etc/kubernetes/ssl/ca.pem \\\\ -etcd-certfile=/etc/flanneld/ssl/flanneld.pem \\\\ -etcd-keyfile=/etc/flanneld/ssl/flanneld-key.pem \\\\ -etcd-endpoints=$&#123;ETCD_ENDPOINTS&#125; \\\\ -etcd-prefix=$&#123;FLANNEL_ETCD_PREFIX&#125;ExecStartPost=/root/local/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/dockerRestart=on-failure[Install]WantedBy=multi-user.targetRequiredBy=docker.serviceEOF mk-docker-opts.sh 脚本将分配给 flanneld 的 Pod 子网网段信息写入到 /run/flannel/docker 文件中，后续 docker 启动时使用这个文件中参数值设置 docker0 网桥； -iface 选项值指定 flanneld 和其它 Node 通信的接口，如果机器有内、外网，则最好指定为内网接口； 完整 unit 见 flanneld.service 启动 flanneld123456$ sudo cp flanneld.service /etc/systemd/system/$ sudo systemctl daemon-reload$ sudo systemctl enable flanneld$ sudo systemctl start flanneld$ systemctl status flanneld$ 检查 flanneld 服务123$ journalctl -u flanneld |grep 'Lease acquired'$ ifconfig flannel.1$ 检查分配给各 flanneld 的 Pod 网段信息123456789101112131415161718192021222324$ # 查看集群 Pod 网段(/16)$ /root/local/bin/etcdctl \\ --endpoints=$&#123;ETCD_ENDPOINTS&#125; \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --cert-file=/etc/flanneld/ssl/flanneld.pem \\ --key-file=/etc/flanneld/ssl/flanneld-key.pem \\ get $&#123;FLANNEL_ETCD_PREFIX&#125;/config&#123; \"Network\": \"172.30.0.0/16\", \"SubnetLen\": 24, \"Backend\": &#123; \"Type\": \"vxlan\" &#125; &#125;$ # 查看已分配的 Pod 子网段列表(/24)$ /root/local/bin/etcdctl \\ --endpoints=$&#123;ETCD_ENDPOINTS&#125; \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --cert-file=/etc/flanneld/ssl/flanneld.pem \\ --key-file=/etc/flanneld/ssl/flanneld-key.pem \\ ls $&#123;FLANNEL_ETCD_PREFIX&#125;/subnets/kubernetes/network/subnets/172.30.19.0-24$ # 查看某一 Pod 网段对应的 flanneld 进程监听的 IP 和网络参数$ /root/local/bin/etcdctl \\ --endpoints=$&#123;ETCD_ENDPOINTS&#125; \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --cert-file=/etc/flanneld/ssl/flanneld.pem \\ --key-file=/etc/flanneld/ssl/flanneld-key.pem \\ get $&#123;FLANNEL_ETCD_PREFIX&#125;/subnets/172.30.19.0-24&#123;\"PublicIP\":\"10.64.3.7\",\"BackendType\":\"vxlan\",\"BackendData\":&#123;\"VtepMAC\":\"d6:51:2e:80:5c:69\"&#125;&#125; 确保各节点间 Pod 网段能互联互通在各节点上部署完 Flannel 后，查看已分配的 Pod 子网段列表(/24) 123456789$ /root/local/bin/etcdctl \\ --endpoints=$&#123;ETCD_ENDPOINTS&#125; \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --cert-file=/etc/flanneld/ssl/flanneld.pem \\ --key-file=/etc/flanneld/ssl/flanneld-key.pem \\ ls $&#123;FLANNEL_ETCD_PREFIX&#125;/subnets/kubernetes/network/subnets/172.30.19.0-24/kubernetes/network/subnets/172.30.20.0-24/kubernetes/network/subnets/172.30.21.0-24 当前三个节点分配的 Pod 网段分别是：172.30.19.0-24、172.30.20.0-24、172.30.21.0-24。 在各节点上分配 ping 这三个网段的网关地址，确保能通： 1234$ ping 172.30.19.1$ ping 172.30.20.2$ ping 172.30.21.3$","categories":[],"tags":[]},{"title":"","slug":"04-部署Kubectl命令行工具","date":"2017-10-27T01:45:16.456Z","updated":"2017-10-27T01:53:17.117Z","comments":true,"path":"2017/10/27/04-部署Kubectl命令行工具/","link":"","permalink":"http://yoursite.com/2017/10/27/04-部署Kubectl命令行工具/","excerpt":"","text":"04-部署Kubectl命令行工具tags: kubectl 部署 kubectl 命令行工具kubectl 默认从 ~/.kube/config 配置文件获取访问 kube-apiserver 地址、证书、用户名等信息，如果没有配置该文件，执行命令时出错： 12$ kubectl get podsThe connection to the server localhost:8080 was refused - did you specify the right host or port? 本文档介绍下载和配置 kubernetes 集群命令行工具 kubectl 的步骤。 需要将下载的 kubectl 二进制程序和生成的 ~/.kube/config 配置文件拷贝到所有使用 kubectl 命令的机器。 使用的变量本文档用到的变量定义如下： 123$ export MASTER_IP=10.64.3.7 # 替换为 kubernetes maste 集群任一机器 IP$ export KUBE_APISERVER=\"https://$&#123;MASTER_IP&#125;:6443\"$ 变量 KUBE_APISERVER 指定 kubelet 访问的 kube-apiserver 的地址，后续被写入 ~/.kube/config 配置文件； 下载 kubectl123456$ wget https://dl.k8s.io/v1.6.2/kubernetes-client-linux-amd64.tar.gz$ tar -xzvf kubernetes-client-linux-amd64.tar.gz$ sudo cp kubernetes/client/bin/kube* /root/local/bin/$ chmod a+x /root/local/bin/kube*$ export PATH=/root/local/bin:$PATH$ 创建 admin 证书kubectl 与 kube-apiserver 的安全端口通信，需要为安全通信提供 TLS 证书和秘钥。 创建 admin 证书签名请求 123456789101112131415161718$ cat admin-csr.json&#123; \"CN\": \"admin\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"system:masters\", \"OU\": \"System\" &#125; ]&#125; 后续 kube-apiserver 使用 RBAC 对客户端(如 kubelet、kube-proxy、Pod)请求进行授权； kube-apiserver 预定义了一些 RBAC 使用的 RoleBindings，如 cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，该 Role 授予了调用kube-apiserver 所有 API的权限； O 指定该证书的 Group 为 system:masters，kubelet 使用该证书访问 kube-apiserver 时 ，由于证书被 CA 签名，所以认证通过，同时由于证书用户组为经过预授权的 system:masters，所以被授予访问所有 API 的权限； hosts 属性值为空列表； 生成 admin 证书和私钥： 123456789$ cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/etc/kubernetes/ssl/ca-config.json \\ -profile=kubernetes admin-csr.json | cfssljson -bare admin$ ls admin*admin.csr admin-csr.json admin-key.pem admin.pem$ sudo mv admin*.pem /etc/kubernetes/ssl/$ rm admin.csr admin-csr.json$ 创建 kubectl kubeconfig 文件12345678910111213141516$ # 设置集群参数$ kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125;$ # 设置客户端认证参数$ kubectl config set-credentials admin \\ --client-certificate=/etc/kubernetes/ssl/admin.pem \\ --embed-certs=true \\ --client-key=/etc/kubernetes/ssl/admin-key.pem$ # 设置上下文参数$ kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin$ # 设置默认上下文$ kubectl config use-context kubernetes admin.pem 证书 O 字段值为 system:masters，kube-apiserver 预定义的 RoleBinding cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，该 Role 授予了调用kube-apiserver 相关 API 的权限； 生成的 kubeconfig 被保存到 ~/.kube/config 文件； 分发 kubeconfig 文件将 ~/.kube/config 文件拷贝到运行 kubelet 命令的机器的 ~/.kube/ 目录下。","categories":[],"tags":[]},{"title":"","slug":"03-部署高可用Etcd集群","date":"2017-10-27T01:45:16.444Z","updated":"2017-10-27T01:53:07.547Z","comments":true,"path":"2017/10/27/03-部署高可用Etcd集群/","link":"","permalink":"http://yoursite.com/2017/10/27/03-部署高可用Etcd集群/","excerpt":"","text":"03-部署高可用Etcd集群tags: etcd 部署高可用 etcd 集群kuberntes 系统使用 etcd 存储所有数据，本文档介绍部署一个三节点高可用 etcd 集群的步骤，这三个节点复用 kubernetes master 机器，分别命名为etcd-host0、etcd-host1、etcd-host2： etcd-host0：10.64.3.7 etcd-host1：10.64.3.8 etcd-host2：10.66.3.86 使用的变量本文档用到的变量定义如下： 12345678$ export NODE_NAME=etcd-host0 # 当前部署的机器名称(随便定义，只要能区分不同机器即可)$ export NODE_IP=10.64.3.7 # 当前部署的机器 IP$ export NODE_IPS=\"10.64.3.7 10.64.3.8 10.66.3.86\" # etcd 集群所有机器 IP$ # etcd 集群间通信的IP和端口$ export ETCD_NODES=etcd-host0=https://10.64.3.7:2380,etcd-host1=https://10.64.3.8:2380,etcd-host2=https://10.66.3.86:2380$ # 导入用到的其它全局变量：ETCD_ENDPOINTS、FLANNEL_ETCD_PREFIX、CLUSTER_CIDR$ source /root/local/bin/environment.sh$ 下载二进制文件到 https://github.com/coreos/etcd/releases 页面下载最新版本的二进制文件： 1234$ wget https://github.com/coreos/etcd/releases/download/v3.1.6/etcd-v3.1.6-linux-amd64.tar.gz$ tar -xvf etcd-v3.1.6-linux-amd64.tar.gz$ sudo mv etcd-v3.1.6-linux-amd64/etcd* /root/local/bin$ 创建 TLS 秘钥和证书为了保证通信安全，客户端(如 etcdctl) 与 etcd 集群、etcd 集群之间的通信需要使用 TLS 加密，本节创建 etcd TLS 加密所需的证书和私钥。 创建 etcd 证书签名请求： 12345678910111213141516171819202122$ cat &gt; etcd-csr.json &lt;&lt;EOF&#123; \"CN\": \"etcd\", \"hosts\": [ \"127.0.0.1\", \"$&#123;NODE_IP&#125;\" ], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125;EOF hosts 字段指定授权使用该证书的 etcd 节点 IP； 生成 etcd 证书和私钥： 123456789$ cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/etc/kubernetes/ssl/ca-config.json \\ -profile=kubernetes etcd-csr.json | cfssljson -bare etcd$ ls etcd*etcd.csr etcd-csr.json etcd-key.pem etcd.pem$ sudo mkdir -p /etc/etcd/ssl$ sudo mv etcd*.pem /etc/etcd/ssl$ rm etcd.csr etcd-csr.json 创建 etcd 的 systemd unit 文件1234567891011121314151617181920212223242526272829303132333435$ sudo mkdir -p /var/lib/etcd # 必须先创建工作目录$ cat &gt; etcd.service &lt;&lt;EOF[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.targetDocumentation=https://github.com/coreos[Service]Type=notifyWorkingDirectory=/var/lib/etcd/ExecStart=/root/local/bin/etcd \\\\ --name=$&#123;NODE_NAME&#125; \\\\ --cert-file=/etc/etcd/ssl/etcd.pem \\\\ --key-file=/etc/etcd/ssl/etcd-key.pem \\\\ --peer-cert-file=/etc/etcd/ssl/etcd.pem \\\\ --peer-key-file=/etc/etcd/ssl/etcd-key.pem \\\\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\\\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\\\ --initial-advertise-peer-urls=https://$&#123;NODE_IP&#125;:2380 \\\\ --listen-peer-urls=https://$&#123;NODE_IP&#125;:2380 \\\\ --listen-client-urls=https://$&#123;NODE_IP&#125;:2379,http://127.0.0.1:2379 \\\\ --advertise-client-urls=https://$&#123;NODE_IP&#125;:2379 \\\\ --initial-cluster-token=etcd-cluster-0 \\\\ --initial-cluster=$&#123;ETCD_NODES&#125; \\\\ --initial-cluster-state=new \\\\ --data-dir=/var/lib/etcdRestart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF 指定 etcd 的工作目录和数据目录为 /var/lib/etcd，需在启动服务前创建这个目录； 为了保证通信安全，需要指定 etcd 的公私钥(cert-file和key-file)、Peers 通信的公私钥和 CA 证书(peer-cert-file、peer-key-file、peer-trusted-ca-file)、客户端的CA证书（trusted-ca-file）； --initial-cluster-state 值为 new 时，--name 的参数值必须位于 --initial-cluster 列表中； 完整 unit 文件见：etcd.service 启动 etcd 服务123456$ sudo mv etcd.service /etc/systemd/system/$ sudo systemctl daemon-reload$ sudo systemctl enable etcd$ sudo systemctl start etcd$ systemctl status etcd$ 最先启动的 etcd 进程会卡住一段时间，等待其它节点上的 etcd 进程加入集群，为正常现象。 在所有的 etcd 节点重复上面的步骤，直到所有机器的 etcd 服务都已启动。 验证服务部署完 etcd 集群后，在任一 etcd 集群节点上执行如下命令： 1234567$ for ip in $&#123;NODE_IPS&#125;; do ETCDCTL_API=3 /root/local/bin/etcdctl \\ --endpoints=https://$&#123;ip&#125;:2379 \\ --cacert=/etc/kubernetes/ssl/ca.pem \\ --cert=/etc/etcd/ssl/etcd.pem \\ --key=/etc/etcd/ssl/etcd-key.pem \\ endpoint health; done 预期结果： 1234562017-04-10 14:50:50.011317 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecatedhttps://10.64.3.7:2379 is healthy: successfully committed proposal: took = 1.687897ms2017-04-10 14:50:50.061577 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecatedhttps://10.64.3.8:2379 is healthy: successfully committed proposal: took = 1.246915ms2017-04-10 14:50:50.104718 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecatedhttps://10.66.3.86:2379 is healthy: successfully committed proposal: took = 1.509229ms 三台 etcd 的输出均为 healthy 时表示集群服务正常（忽略 warning 信息）。","categories":[],"tags":[]},{"title":"","slug":"02-创建CA证书和秘钥","date":"2017-10-27T01:45:16.434Z","updated":"2017-10-27T01:53:07.547Z","comments":true,"path":"2017/10/27/02-创建CA证书和秘钥/","link":"","permalink":"http://yoursite.com/2017/10/27/02-创建CA证书和秘钥/","excerpt":"","text":"02-创建CA证书和秘钥tags: TLS, CA 创建 CA 证书和秘钥kubernetes 系统各组件需要使用 TLS 证书对通信进行加密，本文档使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 证书和秘钥文件，CA 是自签名的证书，用来签名后续创建的其它 TLS 证书。 安装 CFSSL123456789101112131415161718$ wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64$ chmod +x cfssl_linux-amd64$ sudo mv cfssl_linux-amd64 /root/local/bin/cfssl$ wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64$ chmod +x cfssljson_linux-amd64$ sudo mv cfssljson_linux-amd64 /root/local/bin/cfssljson$ wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64$ chmod +x cfssl-certinfo_linux-amd64$ sudo mv cfssl-certinfo_linux-amd64 /root/local/bin/cfssl-certinfo$ export PATH=/root/local/bin:$PATH$ mkdir ssl$ cd ssl$ cfssl print-defaults config &gt; config.json$ cfssl print-defaults csr &gt; csr.json$ 创建 CA (Certificate Authority)创建 CA 配置文件： 12345678910111213141516171819$ cat ca-config.json&#123; \"signing\": &#123; \"default\": &#123; \"expiry\": \"8760h\" &#125;, \"profiles\": &#123; \"kubernetes\": &#123; \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"8760h\" &#125; &#125; &#125;&#125; ca-config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile； signing：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE； server auth：表示 client 可以用该 CA 对 server 提供的证书进行验证； client auth：表示 server 可以用该 CA 对 client 提供的证书进行验证； 创建 CA 证书签名请求： 1234567891011121314151617$ cat ca-csr.json&#123; \"CN\": \"kubernetes\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125; “CN”：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法； “O”：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)； 生成 CA 证书和私钥： 1234$ cfssl gencert -initca ca-csr.json | cfssljson -bare ca$ ls ca*ca-config.json ca.csr ca-csr.json ca-key.pem ca.pem$ 分发证书将生成的 CA 证书、秘钥文件、配置文件拷贝到所有机器的 /etc/kubernetes/ssl 目录下 123$ sudo mkdir -p /etc/kubernetes/ssl$ sudo cp ca* /etc/kubernetes/ssl$ 校验证书以校验 kubernetes 证书(后续部署 master 节点时生成的)为例： 使用 openssl 命令123456789101112131415161718192021222324$ openssl x509 -noout -text -in kubernetes.pem... Signature Algorithm: sha256WithRSAEncryption Issuer: C=CN, ST=BeiJing, L=BeiJing, O=k8s, OU=System, CN=Kubernetes Validity Not Before: Apr 5 05:36:00 2017 GMT Not After : Apr 5 05:36:00 2018 GMT Subject: C=CN, ST=BeiJing, L=BeiJing, O=k8s, OU=System, CN=kubernetes... X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Basic Constraints: critical CA:FALSE X509v3 Subject Key Identifier: DD:52:04:43:10:13:A9:29:24:17:3A:0E:D7:14:DB:36:F8:6C:E0:E0 X509v3 Authority Key Identifier: keyid:44:04:3B:60:BD:69:78:14:68:AF:A0:41:13:F6:17:07:13:63:58:CD X509v3 Subject Alternative Name: DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster, DNS:kubernetes.default.svc.cluster.local, IP Address:127.0.0.1, IP Address:10.64.3.7, IP Address:10.254.0.1... 确认 Issuer 字段的内容和 ca-csr.json 一致； 确认 Subject 字段的内容和 kubernetes-csr.json 一致； 确认 X509v3 Subject Alternative Name 字段的内容和 kubernetes-csr.json 一致； 确认 X509v3 Key Usage、Extended Key Usage 字段的内容和 ca-config.json 中 kubernetes profile 一致； 使用 cfssl-certinfo 命令12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152$ cfssl-certinfo -cert kubernetes.pem...&#123; \"subject\": &#123; \"common_name\": \"kubernetes\", \"country\": \"CN\", \"organization\": \"k8s\", \"organizational_unit\": \"System\", \"locality\": \"BeiJing\", \"province\": \"BeiJing\", \"names\": [ \"CN\", \"BeiJing\", \"BeiJing\", \"k8s\", \"System\", \"kubernetes\" ] &#125;, \"issuer\": &#123; \"common_name\": \"Kubernetes\", \"country\": \"CN\", \"organization\": \"k8s\", \"organizational_unit\": \"System\", \"locality\": \"BeiJing\", \"province\": \"BeiJing\", \"names\": [ \"CN\", \"BeiJing\", \"BeiJing\", \"k8s\", \"System\", \"Kubernetes\" ] &#125;, \"serial_number\": \"174360492872423263473151971632292895707129022309\", \"sans\": [ \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\", \"127.0.0.1\", \"10.64.3.7\", \"10.64.3.8\", \"10.66.3.86\", \"10.254.0.1\" ], \"not_before\": \"2017-04-05T05:36:00Z\", \"not_after\": \"2018-04-05T05:36:00Z\", \"sigalg\": \"SHA256WithRSA\",... 参考 Generate self-signed certificates Setting up a Certificate Authority and Creating TLS Certificates Client Certificates V/s Server Certificates","categories":[],"tags":[]},{"title":"","slug":"01-组件版本和集群环境","date":"2017-10-27T01:45:16.415Z","updated":"2017-10-27T01:53:07.547Z","comments":true,"path":"2017/10/27/01-组件版本和集群环境/","link":"","permalink":"http://yoursite.com/2017/10/27/01-组件版本和集群环境/","excerpt":"","text":"01-组件版本和集群环境tags: kubernetes, environment 组件版本和集群环境集群组件和版本 Kubernetes 1.6.2 Docker 17.04.0-ce Etcd 3.1.6 Flanneld 0.7.1 vxlan 网络 TLS 认证通信 (所有组件，如 etcd、kubernetes master 和 node) RBAC 授权 kubelet TLS BootStrapping kubedns、dashboard、heapster (influxdb、grafana)、EFK (elasticsearch、fluentd、kibana) 插件 私有 docker registry，使用 ceph rgw 后端存储，TLS + HTTP Basic 认证 集群机器 10.64.3.7 10.64.3.8 10.66.3.86 本着测试的目的，etcd 集群、kubernetes master 集群、kubernetes node 均使用这三台机器。 集群环境变量后续的部署步骤将使用下面定义的全局环境变量，根据自己的机器、网络情况修改： 12345678910111213141516171819202122232425262728# TLS Bootstrapping 使用的 Token，可以使用命令 head -c 16 /dev/urandom | od -An -t x | tr -d ' ' 生成BOOTSTRAP_TOKEN=\"41f7e4ba8b7be874fcff18bf5cf41a7c\"# 建议用 未用的网段 来定义服务网段和 Pod 网段# 服务网段 (Service CIDR），部署前路由不可达，部署后集群内使用 IP:Port 可达SERVICE_CIDR=\"10.254.0.0/16\"# POD 网段 (Cluster CIDR），部署前路由不可达，**部署后**路由可达 (flanneld 保证)CLUSTER_CIDR=\"172.30.0.0/16\"# 服务端口范围 (NodePort Range)NODE_PORT_RANGE=\"8400-9000\"# etcd 集群服务地址列表ETCD_ENDPOINTS=\"https://10.64.3.7:2379,https://10.64.3.8:2379,https://10.66.3.86:2379\"# flanneld 网络配置前缀FLANNEL_ETCD_PREFIX=\"/kubernetes/network\"# kubernetes 服务 IP (预分配，一般是 SERVICE_CIDR 中第一个IP)CLUSTER_KUBERNETES_SVC_IP=\"10.254.0.1\"# 集群 DNS 服务 IP (从 SERVICE_CIDR 中预分配)CLUSTER_DNS_SVC_IP=\"10.254.0.2\"# 集群 DNS 域名CLUSTER_DNS_DOMAIN=\"cluster.local.\" 打包后的变量定义见 environment.sh，后续部署时会提示导入该脚本； 分发集群环境变量定义脚本把全局变量定义脚本拷贝到所有机器的 /root/local/bin 目录： 12$ cp environment.sh /root/local/bin$","categories":[],"tags":[]}]}